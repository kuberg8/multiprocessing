<!-- mpiexec -n 4 python3 pi2.py    -->
mpiexec -n <PROCESS_COUNT> python3 <FILENAME>.py


# pi.py
Этот код является примером параллельной реализации метода Монте-Карло для оценки значения Pi с использованием библиотеки Message Passing Interface (MPI) в Python (mpi4py).

Программа разделена на две части: ведущий узел (ранг 0) и ведомые узлы (ранг > 0). Ведущий узел отвечает за координацию вычислений, распределяя фрагменты работы между ведомыми узлами и собирая от них результаты. Ведомые узлы выполняют фактические вычисления по фрагментам.

Вот краткий обзор того, как работает код:

1. Код начинается с импорта необходимого модуля mpi4py и инициализации среды MPI.

2. Задаются переменные slice_size и total_slices, определяющие размер каждого среза работы и общее количество срезов, которые необходимо выполнить.

3. На главном узле (ранг 0) переменная pi инициализируется в 0, а переменные slice и process инициализируются в 0 и 1 соответственно. В переменной size хранится общее количество процессов.

4. Затем ведущий узел входит в цикл, чтобы отправить первую партию процессов на ведомые узлы. Он использует функцию comm.send() для отправки значения slice в ранг process с меткой 1. Значение slice представляет собой индекс среза, который будет вычисляться ведомым узлом.

5. После отправки первой партии процессов ведущий узел входит в другой цикл, чтобы дождаться возвращения данных от ведомых узлов. Он использует функцию comm.recv() с MPI.ANY_SOURCE в качестве ранга источника и меткой 1, чтобы получить вычисленное значение slice_value от ведомого узла. Он также получает ранг процесса от ведомого узла, чтобы отслеживать, к какому срезу принадлежат данные.

6. Полученные данные добавляются в переменную pi, а количество полученных процессов увеличивается. Если осталось вычислить еще несколько срезов, ведущий узел отправляет еще один срез тому же ведомому узлу, который только что закончил обработку. Этот процесс продолжается до тех пор, пока не будут вычислены все срезы.

7. Когда все срезы вычислены и получены, ведущий узел посылает сигнал выключения (значение -1) всем ведомым узлам с помощью функции comm.send() с меткой 1.

8. Наконец, ведущий узел печатает расчетное значение Pi, умножая переменную pi на 4,0.

9. В ведомых узлах код входит в бесконечный цикл, пока не получит сигнал выключения (значение -1) от ведущего узла. Внутри цикла он получает начальное значение (индекс вычисляемого фрагмента) от ведущего узла с помощью функции comm.recv().

10. Затем ведомый узел выполняет вычисление среза, используя простую формулу суммирования, основанную на методе Монте-Карло. Вычисленное значение slice_value отправляется обратно ведущему узлу с помощью функции comm.send() с тегом 1. Он также отправляет свой собственный ранг (rank) обратно ведущему узлу с тегом 2, чтобы отслеживать, к какому срезу принадлежат данные.

11. После отправки вычисленного значения среза и ранга ведомый узел возвращается в начало цикла, чтобы проверить сигнал выключения.

Это высокоуровневый обзор того, как работает код. Это параллельная реализация, которая распределяет работу между несколькими процессами с помощью MPI. Каждый процесс (как ведущий, так и ведомый узлы) выполняет свою часть вычислений и обменивается данными друг с другом с помощью MPI-функций comm.send() и comm.recv().

Расчетное значение Pi получается путем суммирования вычисленных значений срезов и умножения результата на 4,0 на ведущем узле. Чем больше срезов вы вычислите, тем точнее будет оценка Pi.

_________________________________________________________________________________________________________________

# pi2.py
Этот код - пример использования MPI (Message Passing Interface) с mpi4py для вычисления значения числа pi методом Монте-Карло. MPI - это популярная библиотека для параллельных вычислений, которая позволяет осуществлять связь и координацию между процессами, запущенными на разных процессорах или узлах.

В этом коде каждый процесс генерирует определенное количество случайных точек и подсчитывает количество точек, которые попадают в единичный круг. Отношение числа точек внутри круга к общему числу точек является оценкой значения pi/4. С помощью операции коллективного взаимодействия MPI Allreduce значение хитов от каждого процесса накапливается и суммируется по всем процессам. Наконец, значение pi вычисляется как (4.0 / size) * total_hits.

Код начинается с инициализации MPI с помощью MPI.COMM_WORLD, которая создает объект-коммуникатор, представляющий группу процессов, участвующих в вычислениях. Ранг текущего процесса получается с помощью comm.Get_rank(), а общее количество процессов - с помощью comm.Get_size().

Функция sample принимает аргумент nsamples, который представляет собой количество генерируемых случайных точек. Она выполняет итерацию nsamples раз, генерируя случайные координаты x и y и сравнивая их расстояние от начала координат, чтобы определить, попадает ли точка в единичный круг. Количество таких точек сохраняется в переменной hits.

Затем создается массив numpy mypi для хранения отношения количества попаданий к общему количеству точек для каждого процесса. Этот массив затем используется в операции comm.Allreduce, которая выполняет операцию уменьшения (в данном случае суммирования) по всем процессам. Результат сохраняется в массиве total.

Наконец, значение pi вычисляется как (4.0 / size) * total[0] и выводится процессом с рангом 0, а также количество использованных процессоров и время, затраченное на вычисления.

Чтобы запустить этот код, вы обычно выполняете его с помощью реализации MPI, такой как OpenMPI или MPICH. Например, используя OpenMPI, вы можете выполнить код командой mpiexec -n <число процессов> python <filename.py>, где <число процессов> - желаемое количество процессов, используемых для вычислений. Обратите внимание, что общее количество образцов делится на количество процессов, чтобы обеспечить равномерное распределение рабочей нагрузки.

_________________________________________________________________________________________________________________

# pi3.py 
В этом примере используется библиотека mpi4py для реализации параллельного выполнения. Коммуникатор comm используется для связи между процессами, получения их рангов и определения общего количества процессов. Каждый процесс генерирует свою долю случайных точек внутри квадрата, используя функцию random.uniform(-1, 1). Затем каждый процесс проверяет, сколько из его точек попадают внутрь единичной окружности и сообщает об этом общему процессу с помощью операции reduce с корневым процессом - процессом с рангом 0. Наконец, корневой процесс суммирует количество точек внутри окружности и получает приближенное значение числа Пи.

При запуске программы с помощью mpiexec -n <число процессов> python <имя файла>.py, каждый процесс будет выполнять свою часть вычислений, а корневой процесс затем выведет приближенное значение числа Пи.

_________________________________________________________________________________________________________________