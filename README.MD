<!-- mpiexec -n 4 python3 pi2.py    -->
mpiexec -n <PROCESS_COUNT> python3 <FILENAME>.py


# pi.py
Этот код является примером параллельной реализации метода Монте-Карло для оценки значения Pi с использованием библиотеки Message Passing Interface (MPI) в Python (mpi4py).

Программа разделена на две части: ведущий узел (ранг 0) и ведомые узлы (ранг > 0). Ведущий узел отвечает за координацию вычислений, распределяя фрагменты работы между ведомыми узлами и собирая от них результаты. Ведомые узлы выполняют фактические вычисления по фрагментам.

Вот краткий обзор того, как работает код:

1. Код начинается с импорта необходимого модуля mpi4py и инициализации среды MPI.

2. Задаются переменные slice_size и total_slices, определяющие размер каждого среза работы и общее количество срезов, которые необходимо выполнить.

3. На главном узле (ранг 0) переменная pi инициализируется в 0, а переменные slice и process инициализируются в 0 и 1 соответственно. В переменной size хранится общее количество процессов.

4. Затем ведущий узел входит в цикл, чтобы отправить первую партию процессов на ведомые узлы. Он использует функцию comm.send() для отправки значения slice в ранг process с меткой 1. Значение slice представляет собой индекс среза, который будет вычисляться ведомым узлом.

5. После отправки первой партии процессов ведущий узел входит в другой цикл, чтобы дождаться возвращения данных от ведомых узлов. Он использует функцию comm.recv() с MPI.ANY_SOURCE в качестве ранга источника и меткой 1, чтобы получить вычисленное значение slice_value от ведомого узла. Он также получает ранг процесса от ведомого узла, чтобы отслеживать, к какому срезу принадлежат данные.

6. Полученные данные добавляются в переменную pi, а количество полученных процессов увеличивается. Если осталось вычислить еще несколько срезов, ведущий узел отправляет еще один срез тому же ведомому узлу, который только что закончил обработку. Этот процесс продолжается до тех пор, пока не будут вычислены все срезы.

7. Когда все срезы вычислены и получены, ведущий узел посылает сигнал выключения (значение -1) всем ведомым узлам с помощью функции comm.send() с меткой 1.

8. Наконец, ведущий узел печатает расчетное значение Pi, умножая переменную pi на 4,0.

9. В ведомых узлах код входит в бесконечный цикл, пока не получит сигнал выключения (значение -1) от ведущего узла. Внутри цикла он получает начальное значение (индекс вычисляемого фрагмента) от ведущего узла с помощью функции comm.recv().

10. Затем ведомый узел выполняет вычисление среза, используя простую формулу суммирования, основанную на методе Монте-Карло. Вычисленное значение slice_value отправляется обратно ведущему узлу с помощью функции comm.send() с тегом 1. Он также отправляет свой собственный ранг (rank) обратно ведущему узлу с тегом 2, чтобы отслеживать, к какому срезу принадлежат данные.

11. После отправки вычисленного значения среза и ранга ведомый узел возвращается в начало цикла, чтобы проверить сигнал выключения.

Это высокоуровневый обзор того, как работает код. Это параллельная реализация, которая распределяет работу между несколькими процессами с помощью MPI. Каждый процесс (как ведущий, так и ведомый узлы) выполняет свою часть вычислений и обменивается данными друг с другом с помощью MPI-функций comm.send() и comm.recv().

Расчетное значение Pi получается путем суммирования вычисленных значений срезов и умножения результата на 4,0 на ведущем узле. Чем больше срезов вы вычислите, тем точнее будет оценка Pi.

_________________________________________________________________________________________________________________

# pi2.py
Этот код - пример использования MPI (Message Passing Interface) с mpi4py для вычисления значения числа pi методом Монте-Карло. MPI - это популярная библиотека для параллельных вычислений, которая позволяет осуществлять связь и координацию между процессами, запущенными на разных процессорах или узлах.

В этом коде каждый процесс генерирует определенное количество случайных точек и подсчитывает количество точек, которые попадают в единичный круг. Отношение числа точек внутри круга к общему числу точек является оценкой значения pi/4. С помощью операции коллективного взаимодействия MPI Allreduce значение хитов от каждого процесса накапливается и суммируется по всем процессам. Наконец, значение pi вычисляется как (4.0 / size) * total_hits.

Код начинается с инициализации MPI с помощью MPI.COMM_WORLD, которая создает объект-коммуникатор, представляющий группу процессов, участвующих в вычислениях. Ранг текущего процесса получается с помощью comm.Get_rank(), а общее количество процессов - с помощью comm.Get_size().

Функция getPi принимает аргумент nsamples, который представляет собой количество генерируемых случайных точек. Она выполняет итерацию nsamples раз, генерируя случайные координаты x и y и сравнивая их расстояние от начала координат, чтобы определить, попадает ли точка в единичный круг. Количество таких точек сохраняется в переменной hits.

Затем создается массив numpy mypi для хранения отношения количества попаданий к общему количеству точек для каждого процесса. Этот массив затем используется в операции comm.Allreduce, которая выполняет операцию уменьшения (в данном случае суммирования) по всем процессам. Результат сохраняется в массиве total.

Наконец, значение pi вычисляется как (4.0 / size) * total[0] и выводится процессом с рангом 0, а также количество использованных процессоров и время, затраченное на вычисления.

Чтобы запустить этот код, вы обычно выполняете его с помощью реализации MPI, такой как OpenMPI или MPICH. Например, используя OpenMPI, вы можете выполнить код командой mpiexec -n <число процессов> python <filename.py>, где <число процессов> - желаемое количество процессов, используемых для вычислений. Обратите внимание, что общее количество образцов делится на количество процессов, чтобы обеспечить равномерное распределение рабочей нагрузки.

_________________________________________________________________________________________________________________

# pi3.py

Данный код использует библиотеку mpi4py для расчета значения числа Пи с использованием параллельных вычислений на нескольких процессах.

Основной код находится в функции getPi(n), которая принимает количество итераций n и выполняет вычисления на каждом процессе.

Вначале происходит распределение работы между процессами. Каждый процесс вычисляет свою часть отрезка значений и суммирует полученные результаты. Затем происходит сбор сумм с помощью операции reduce и отправка суммарного значения на процесс с рангом 0.

Формула Мачина, которая используется в данном коде, представляет собой метод для вычисления значения числа Пи. 

Формула Мачина основывается на разложении арктангенса в бесконечную сумму и имеет следующий вид:

1. π/4 = 4 * (1/1 - 1/3 + 1/5 - 1/7 + 1/9 - 1/11 + ...)
В данной формуле использованы только положительные значения рациональных чисел, это обеспечивает сходимость к значению π/4.

2. Формула Мачина используется для вычисления промежуточной суммы, которая затем будет использоваться для получения окончательного значения числа Пи. Каждый процесс выполняет свою часть итераций, а затем суммирует свои результаты с помощью функции MPI_Reduce или MPI_Allreduce для получения общей суммы от всех процессов.

3. В строке кода x = (i + 0.5) / n вычисляется значение переменной x, которая представляет собой смещение в пределах интервала i до i+1 и позволяет учитывать середину каждого интервала.
Затем в коде используется операция суммирования subtotal += 1 + x**2 для вычисления суммы членов ряда формулы Мачина.

4. В результате в каждом процессе будет вычислена своя промежуточная сумма, и затем эти значения будут суммированы с использованием функции MPI_Reduce или MPI_Allreduce для получения окончательного значения числа Пи.

На процессе с рангом 0 значение числа Пи делится на общее количество итераций n для получения окончательного результата.

В основной части программы текущий процесс с рангом 0 запрашивает количество итераций от пользователя и распространяет это значение на все процессы с помощью функции bcast. Затем вызывается функция getPi(n) для вычисления значения числа Пи. На процессе с рангом 0 выводится окончательный результат.